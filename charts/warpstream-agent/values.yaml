image:
  repository: "public.ecr.aws/warpstream-labs/warpstream_agent"
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: ""

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

# Ignored if auto-scaling is enabled (which it is by default).
replicas: 3

# Only use StatefulSet when TLS is needed for an easier time managing certificates due to stable hostnames.
#
# *Note* that if you switch to using StatefulSet you should stop using WarpStream's convenience bootstrap
# URL in your Kafka clients, and switch to the Kubernetes service name instead. The reason for this is that
# when you switch to StatefulSet, the Agents will begin advertising their stable pod names in the Kafka
# protocol as their hostname, instead of their internal IP addresses. When this happens, WarpStream's DNS
# server will start returning CNAME's for the convience bootstrap URL for each Agent. Since these CNAME's
# are private to the Kubernetes cluster WarpStream's DNS server cannot resolve them causing applications
# to fail to connect.
#
# In general, we always recommend using the Kubernetes service name as the bootstrap URL for your clients
# when the Agents are deployed in Kubernetes to avoid issues like this.
deploymentKind: Deployment  # or StatefulSet

# Only used in Deployments
revisionHistoryLimit: 10

deploymentStrategy:
  type: RollingUpdate
  rollingUpdate:
    # The number of pods that can be created above the desired amount of pods during an update (both integers and percentages are supported)
    maxSurge: 1
    # The number of pods that can be unavailable during the update process (both integers and percentages are supported)
    maxUnavailable: 1

statefulSetConfig:
  clusterDomain: cluster.local
  podManagementPolicy: Parallel  # or OrderedReady, see https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-management-policies
  updateStrategy:
    # see https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies
    type: RollingUpdate

certificate:
  # Set to true to enable TLS termination on the WarpStream agent
  # see TLS documentation for details https://docs.warpstream.com/warpstream/byoc/advanced-agent-deployment-options/protect-data-in-motion-with-tls-encryption#tls-encryption-overview
  enableTLS: false

  # The Kubernetes TLS secret that contains a certificate and private key
  # see https://kubernetes.io/docs/concepts/configuration/secret/#tls-secrets
  secretName: ""

  mtls:
    # Set to true to enable mutual TLS on the WarpStream agent, requires enableTLS set to true
    # see mTLS documentation for details https://docs.warpstream.com/warpstream/byoc/authentication/mutual-tls-mtls
    enabled: false

    # The secret key reference for the certificate authority public key
    # If not set, the container's root certificate pool will be used for mtls client certificate verification
    # It is recommend to always set this to your CA's public key and not rely on the root certificate pool
    # see mTLS documentation for details on why using the root certificate pool is not recommended
    certificateAuthoritySecretKeyRef:
      name: ""
      key: ""

terminationGracePeriodSeconds: 300

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Specifies whether a service token should be mounted on pod
  automountServiceAccountToken: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  # name: ""

securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # runAsNonRoot: true

service:
  type: ClusterIP
  port: 9092
  httpPort: 8080
  schemaRegistryPort: 9094
  labels: {}

  # If specified and supported by the platform, this will restrict traffic through the cloud-provider load-balancer will be restricted
  # to the specified client IPs. This field will be ignored if the cloud-provider does not support the feature.
  # More info: https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/
  # loadBalancerSourceRanges: []

  # Create services per pod, this can be used in combination with deploymentKind=StatefulSet and statefulSetConfig.clusterDomain
  # to expose each pod outside of the cluster when using Kong or Isto ingresses.
  # See https://github.com/warpstreamlabs/charts/pull/156 for details
  perPod:
    enabled: false

# A service that only contains the Kafka TCP port
# Typically used to help with exposing only Kafka via a Load Balancer
kafkaService:
  enabled: false
  type: ClusterIP
  port: 9092

  # If specified and supported by the platform, this will restrict traffic through the cloud-provider load-balancer will be restricted
  # to the specified client IPs. This field will be ignored if the cloud-provider does not support the feature.
  # More info: https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/
  # loadBalancerSourceRanges: []

rbac:
  create: true

headlessService:
  enabled: true

config:
  # If set to false, configmap will not be created
  # When the configmap is not created, the environment variables WARPSTREAM_BUCKET_URL, WARPSTREAM_DEFAULT_VIRTUAL_CLUSTER_ID, and WARPSTREAM_REGION
  # should be set via `extraEnv` or `extraEnvFrom`
  configMapEnabled: true
  playground: false
  ## To learn what values to set for the config variables, look at our documentation
  ## for configuring the WarpStream Agents for production.
  ## https://docs.warpstream.com/warpstream/how-to/configure-the-warpstream-agent-for-production
  # bucketURL: "mem://mem_bucket"
  # virtualClusterID: "YOUR_VIRTUAL_CLUSTER_ID"
  # region: "us-east-1"
  #
  # Agent API Key Secret
  # secretName: # DEPRECATED this will be removed soon, please switch to agentKeySecretKeyRef
  #
  # DEPRECATED apiKey and apiKeySecretKeyRef will be removed soon, please switch to agentKey or agentKeySecretKeyRef.
  # apiKey: "YOUR_API_KEY" # Provide this if you want the chart to manage the secret
  # apiKeySecretKeyRef: # Provide this if you want to manage the secret externally
  #   name:
  #   key:
  #
  # Agent Key Secret
  # agentKey: "YOUR_AGENT_KEY" # Provide this if you want the chart to manage the secret
  # agentKeySecretKeyRef: # Provide this if you want to manage the secret externally
  #   name:
  #   key:
  #
  # Agent Groups configuration https://docs.warpstream.com/warpstream/byoc/advanced-agent-deployment-options/agent-groups
  # Leave empty for Default group
  agentGroup: ""
  # Low Latency Cluster Settings
  # When these are set bucketURL cannot be set.
  # Ref: https://docs.warpstream.com/warpstream/byoc/advanced-agent-deployment-options/low-latency-clusters
  # The ingestion bucket URL:
  # warpstream_multi://s3://bucket-one-us-east-1a?region=us-east-1<>s3://bucket-one-us-east-1b?region=us-east-1<>s3://bucket-one-us-east-1b?region=us-east-1
  # ingestionBucketURL: ""
  # The compaction bucket URL should be a single classic S3 Bucket following the normal format:
  # s3://my-bucket?region=us-east-1
  # compactionBucketURL: ""

# Ensures that the Agents are configured with the recommended resources for production
# usage. We recommend that you do not set this to false except for in staging/dev
# environments.
enforceProductionResourceRequirements: true
resources:
  # Recommendations:
  #   1. At least 4 requests vCPUs per Agent, and only use whole numbers for CPUs
  #      because we set GOMAXPROCS to the value of requested CPUs.
  #      The Agents use the value of GOMAXPROCS to determine how many cores they have,
  #      (to avoid throttling) and size caches and other data structures in accordance
  #      with the number of available cores (which is assumed to scale linearly with
  #      the amount of available ram with a ratio of at least 4GiB of RAM per vCPU.
  #      Setting GOMAXPROCS lower than the number of requested CPUs would prevent
  #      the Agents from utilizing all of the available resources. Setting the value
  #      higher may lead to OOMs as the Agents try to use more RAM than they actually
  #      have available.
  #   2. At least 4GiB of RAM per vCPU.
  #   3. Run on dedicated network optimized instances (m6in class in AWS) for high volume
  #      use-cases.
  #   4. Keep memory requests and limits the same.
  requests:
    cpu: 4
    memory: 16Gi
    # we do not need the disk space, but Kubernetes will count some logs that it emits
    # about our containers towards our containers ephemeral usage and if we requested
    # 0 storage we could end up getting evicted unnecessarily when the node is under disk pressure.
    ephemeral-storage: "100Mi"
  limits:
    memory: 16Gi
    # We don't limit ephemeral-storage because its coorelated with how many logs the
    # Agents are emitting which is impossible to predict.

extraEnv: []
# Add additional environment settings to the pod. Can be useful in proxy
# environments

extraEnvFrom: []
# Add additional environment from configmaps or secrets.
# Example:
# extraEnvFrom:
#   - configMapRef:
#       name: my-env-from-configmap
#   - secretRef:
#       name: my-env-from-secret

volumeMounts: []
# Add any volumeMounts to the pod.

extraArgs: []
# Add additional args settings to the pod.

nodeSelector: {}

tolerations: []

affinity: {}

## Topology Spread Constraints
## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/
topologySpreadConstraints: []

priorityClassName: ""

# Optional horizontal pod autoscaler.
autoscaling:
  enabled: true
  minReplicas: 3
  maxReplicas: 30
  targetCPU: "60"
  targetMemory: ""
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 1800
      policies:
      - type: Pods
        value: 1
        periodSeconds: 900
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
      - type: Percent
        value: 50
        periodSeconds: 300
      - type: Pods
        value: 3
        periodSeconds: 300
      selectPolicy: Max

# Optional disruption budget. Enabled by default to minimize disruption during deploys.
pdb:
  create: true
  # This *could* be replaced with minAvailable: "90%" for large deployments, however,
  # that can be problematic if az-aware routing is enabled because the PDB is not
  # az-aware and a 10% disruption could mean a loss of 30% capacity in a single zone.
  maxUnavailable: 1

serviceMonitor:
  ## If true, a ServiceMonitor CRD is created for a prometheus operator
  ## https://github.com/coreos/prometheus-operator
  ##
  enabled: false
  path: /metrics
  labels: {}
  interval: 30s
  scheme: http
  tlsConfig: {}
  scrapeTimeout: 30s
  relabelings: []
  metricRelabelings: []
  targetLabels: []
  honorLabels: true

prometheusRule:
  ## If true, a PrometheusRule CRD is created for a prometheus operator
  ## https://github.com/coreos/prometheus-operator
  ##
  enabled: false
  labels: {}
  groups:
    - name: groupName
      rules:
      - alert: alertName
        expr: 'alert expression'
        for: 5m
        labels:
          severity: high
        annotations:
          identifier: 'Alert identifier.'
          msg: Alert message.

## list of hosts and IPs that will be injected into the pod's hosts file
hostAliases: []
  # - ip: "127.0.0.1"
  #   hostnames:
  #   - "foo.local"
  #   - "bar.local"

# Labels to be added to each agent pod
podLabels: {}

# Optional initContainers definition
initContainers: []

# Optional volume definition
volumes: []

## Use an alternate scheduler
## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
##
# schedulerName: default-scheduler

# Resources for test pods
test:
  resources:
    requests:
      cpu: 200m
      memory: 256Mi
    limits:
      cpu: 500m
      memory: 512Mi
  nodeSelector:
    kubernetes.io/arch: amd64
